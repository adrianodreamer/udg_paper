{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper para Plumas Libres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando scraping de 47 páginas (desde 1 hasta 47)\n",
      "Usando 5 workers para paralelización\n",
      "\n",
      "Calculando cantidad total de artículos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando páginas: 100%|██████████| 47/47 [00:10<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de artículos válidos encontrados: 470\n",
      "\n",
      "Iniciando scraping paralelo de artículos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso: 100%|██████████| 470/470 [01:05<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completado en 76.41 segundos\n",
      "Artículos procesados exitosamente: 470/470\n",
      "Artículos fallidos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'Artículo', 'Fecha', 'Título', 'Autor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m max_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Ajustar según necesidad\u001b[39;00m\n\u001b[1;32m    171\u001b[0m scraper\u001b[38;5;241m.\u001b[39mscrape(start_page\u001b[38;5;241m=\u001b[39mstart_page, end_page\u001b[38;5;241m=\u001b[39mend_page, max_workers\u001b[38;5;241m=\u001b[39mmax_workers)\n\u001b[0;32m--> 172\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 159\u001b[0m, in \u001b[0;36mPlumasLibresScraper.save_to_csv\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    157\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n\u001b[1;32m    158\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marticles_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDatos guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticles_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m artículos)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'Artículo', 'Fecha', 'Título', 'Autor'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "\n",
    "class PlumasLibresScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://plumaslibres.com.mx/category/sociedad/page/{}/\"\n",
    "        self.articles_data = []\n",
    "        self.lock = False\n",
    "        self.article_pattern = re.compile(r'https://plumaslibres\\.com\\.mx/\\d{4}/\\d{2}/\\d{2}/.+')\n",
    "        \n",
    "    def is_valid_article_url(self, url):\n",
    "        \"\"\"Verifica si la URL es de un artículo válido\"\"\"\n",
    "        return bool(self.article_pattern.match(url)) and not any(x in url for x in ['/category/', '/author/'])\n",
    "    \n",
    "    def get_article_links(self, page_num):\n",
    "        \"\"\"Obtiene los enlaces a artículos válidos de una página\"\"\"\n",
    "        url = self.base_url.format(page_num)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.encoding = 'utf-8'  # Forzar codificación UTF-8\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            articles = soup.select('main.main div.col-sm-6 a[href]')\n",
    "            valid_links = []\n",
    "            \n",
    "            for a in articles:\n",
    "                href = a['href']\n",
    "                if self.is_valid_article_url(href):\n",
    "                    valid_links.append(href)\n",
    "            \n",
    "            return list(set(valid_links))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError al obtener enlaces de página {page_num}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_article(self, url):\n",
    "        \"\"\"Extrae información de un artículo con manejo robusto de errores\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.encoding = 'utf-8'  # Forzar codificación UTF-8\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Verificar estructura básica del artículo\n",
    "            article_body = soup.find('article')\n",
    "            if not article_body:\n",
    "                return False\n",
    "                \n",
    "            # Extraer título con manejo de error\n",
    "            title_elem = soup.find('h1', class_='entry-title')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"Sin título\"\n",
    "            \n",
    "            # Extraer fecha\n",
    "            date_elem = soup.find('time', class_='updated')\n",
    "            date = date_elem.get_text(strip=True) if date_elem else \"Sin fecha\"\n",
    "            \n",
    "            # Extraer autor\n",
    "            author = \"Sin autor\"\n",
    "            share_div = soup.find('div', string=lambda t: t and \"Compartir con WhatsApp\" in t)\n",
    "            if share_div:\n",
    "                author_tag = share_div.find_next('p')\n",
    "                if author_tag:\n",
    "                    author = author_tag.get_text(strip=True).replace('Por ', '')\n",
    "            \n",
    "            # Extraer contenido\n",
    "            content = []\n",
    "            for p in article_body.find_all('p'):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and not any(x in text for x in ['Compartir con', 'Síguenos en']):\n",
    "                    content.append(text)\n",
    "            \n",
    "            # Simulamos thread safety\n",
    "            while self.lock:\n",
    "                time.sleep(0.1)\n",
    "            self.lock = True\n",
    "            self.articles_data.append({\n",
    "                'URL': url,\n",
    "                'Título': title,\n",
    "                'Artículo': '\\n'.join(content),\n",
    "                'Fecha': date,\n",
    "                'Autor': author\n",
    "            })\n",
    "            self.lock = False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError al scrapear artículo {url}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_page(self, page_num):\n",
    "        \"\"\"Procesa una página completa\"\"\"\n",
    "        article_links = self.get_article_links(page_num)\n",
    "        success_count = 0\n",
    "        \n",
    "        for link in article_links:\n",
    "            if self.scrape_article(link):\n",
    "                success_count += 1\n",
    "        \n",
    "        return (page_num, len(article_links)), success_count\n",
    "    \n",
    "    def scrape(self, start_page=1, end_page=47, max_workers=5):\n",
    "        \"\"\"Función principal con paralelización\"\"\"\n",
    "        total_pages = end_page - start_page + 1\n",
    "        total_articles = 0\n",
    "        processed_articles = 0\n",
    "        \n",
    "        print(f\"\\nIniciando scraping de {total_pages} páginas (desde {start_page} hasta {end_page})\")\n",
    "        print(f\"Usando {max_workers} workers para paralelización\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Primera pasada: conteo total\n",
    "            print(\"Calculando cantidad total de artículos...\")\n",
    "            future_to_page = {\n",
    "                executor.submit(self.get_article_links, page_num): page_num \n",
    "                for page_num in range(start_page, end_page + 1)\n",
    "            }\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_page), total=total_pages, desc=\"Analizando páginas\"):\n",
    "                article_links = future.result()\n",
    "                total_articles += len(article_links)\n",
    "            \n",
    "            print(f\"\\nTotal de artículos válidos encontrados: {total_articles}\\n\")\n",
    "            \n",
    "            # Segunda pasada: scraping real\n",
    "            print(\"Iniciando scraping paralelo de artículos...\")\n",
    "            future_to_page = {\n",
    "                executor.submit(self.scrape_page, page_num): page_num \n",
    "                for page_num in range(start_page, end_page + 1)\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=total_articles, desc=\"Progreso\") as pbar:\n",
    "                for future in as_completed(future_to_page):\n",
    "                    (page_num, page_articles), success_count = future.result()\n",
    "                    pbar.update(page_articles)\n",
    "                    processed_articles += success_count\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nScraping completado en {elapsed_time:.2f} segundos\")\n",
    "        print(f\"Artículos procesados exitosamente: {processed_articles}/{total_articles}\")\n",
    "        print(f\"Artículos fallidos: {total_articles - processed_articles}\")\n",
    "    \n",
    "    def save_to_csv(self, filename=\"plumas_libres_sociedad.csv\"):\n",
    "        \"\"\"Guarda los datos en CSV con codificación UTF-8\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8-sig', newline='') as csvfile:  # Usamos utf-8-sig para BOM\n",
    "            fieldnames = ['URL', 'titulo', 'articulo', 'fecha', 'autor']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.articles_data)\n",
    "        \n",
    "        print(f\"\\nDatos guardados en {filename} (Total: {len(self.articles_data)} artículos)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PlumasLibresScraper()\n",
    "    \n",
    "    # Configuración\n",
    "    start_page = 1\n",
    "    end_page = 47\n",
    "    max_workers = 5  # Ajustar según necesidad\n",
    "    \n",
    "    scraper.scrape(start_page=start_page, end_page=end_page, max_workers=max_workers)\n",
    "    scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando scraping de 47 páginas (desde 1 hasta 47)\n",
      "Usando 7 workers para paralelización\n",
      "\n",
      "Calculando cantidad total de artículos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adc5ca144384a898b4201c7be6ceda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analizando páginas:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de artículos válidos encontrados: 470\n",
      "\n",
      "Iniciando scraping paralelo de artículos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2179fbdbb2944776bc60506ce2242d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progreso:   0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completado en 18.22 segundos\n",
      "Artículos procesados exitosamente: 470/470\n",
      "Artículos fallidos: 0\n",
      "\n",
      "Datos guardados en plumas_libres_sociedad.csv (Total: 470 artículos)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm  # Versión específica para Jupyter\n",
    "import time\n",
    "import re\n",
    "\n",
    "class PlumasLibresScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://plumaslibres.com.mx/category/sociedad/page/{}/\"\n",
    "        self.articles_data = []\n",
    "        self.lock = False\n",
    "        self.article_pattern = re.compile(r'https://plumaslibres\\.com\\.mx/\\d{4}/\\d{2}/\\d{2}/.+')\n",
    "        self.session = requests.Session()  # Mejor manejo de conexiones\n",
    "        \n",
    "    def is_valid_article_url(self, url):\n",
    "        \"\"\"Verifica si la URL es de un artículo válido\"\"\"\n",
    "        return bool(self.article_pattern.match(url)) and not any(x in url for x in ['/category/', '/author/'])\n",
    "    \n",
    "    def get_article_links(self, page_num):\n",
    "        \"\"\"Obtiene los enlaces a artículos válidos de una página\"\"\"\n",
    "        url = self.base_url.format(page_num)\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.encoding = 'utf-8'\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            articles = soup.select('main.main div.col-sm-6 a[href]')\n",
    "            valid_links = []\n",
    "            \n",
    "            for a in articles:\n",
    "                href = a['href']\n",
    "                if self.is_valid_article_url(href):\n",
    "                    valid_links.append(href)\n",
    "            \n",
    "            return list(set(valid_links))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError al obtener enlaces de página {page_num}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_article(self, url):\n",
    "        \"\"\"Extrae información de un artículo con manejo robusto de errores\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.encoding = 'utf-8'\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            article_body = soup.find('article')\n",
    "            if not article_body:\n",
    "                return False\n",
    "                \n",
    "            # Extraer título\n",
    "            title_elem = soup.find('h1', class_='entry-title')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"Sin título\"\n",
    "            \n",
    "            # Extraer fecha\n",
    "            date_elem = soup.find('time', class_='updated')\n",
    "            date = date_elem.get_text(strip=True) if date_elem else \"Sin fecha\"\n",
    "            \n",
    "            # Extraer autor\n",
    "            author = \"Sin autor\"\n",
    "            share_div = soup.find('div', string=lambda t: t and \"Compartir con WhatsApp\" in t)\n",
    "            if share_div:\n",
    "                author_tag = share_div.find_next('p')\n",
    "                if author_tag:\n",
    "                    author = author_tag.get_text(strip=True).replace('Por ', '')\n",
    "            \n",
    "            # Extraer contenido\n",
    "            content = []\n",
    "            for p in article_body.find_all('p'):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and not any(x in text for x in ['Compartir con', 'Síguenos en']):\n",
    "                    content.append(text)\n",
    "            \n",
    "            # Thread safety\n",
    "            while self.lock:\n",
    "                time.sleep(0.1)\n",
    "            self.lock = True\n",
    "            self.articles_data.append({\n",
    "                'url': url,\n",
    "                'titulo': title,\n",
    "                'articulo': '\\n'.join(content),\n",
    "                'fecha': date,\n",
    "                'autor': author\n",
    "            })\n",
    "            self.lock = False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError al scrapear artículo {url}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_page(self, page_num):\n",
    "        \"\"\"Procesa una página completa\"\"\"\n",
    "        article_links = self.get_article_links(page_num)\n",
    "        success_count = 0\n",
    "        \n",
    "        for link in article_links:\n",
    "            if self.scrape_article(link):\n",
    "                success_count += 1\n",
    "        \n",
    "        return (page_num, len(article_links)), success_count\n",
    "    \n",
    "    def scrape(self, start_page=1, end_page=47, max_workers=5):\n",
    "        \"\"\"Función principal con paralelización\"\"\"\n",
    "        total_pages = end_page - start_page + 1\n",
    "        total_articles = 0\n",
    "        processed_articles = 0\n",
    "        \n",
    "        print(f\"\\nIniciando scraping de {total_pages} páginas (desde {start_page} hasta {end_page})\")\n",
    "        print(f\"Usando {max_workers} workers para paralelización\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Primera pasada: conteo total\n",
    "            print(\"Calculando cantidad total de artículos...\")\n",
    "            future_to_page = {\n",
    "                executor.submit(self.get_article_links, page_num): page_num \n",
    "                for page_num in range(start_page, end_page + 1)\n",
    "            }\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_page), total=total_pages, desc=\"Analizando páginas\"):\n",
    "                article_links = future.result()\n",
    "                total_articles += len(article_links)\n",
    "            \n",
    "            print(f\"\\nTotal de artículos válidos encontrados: {total_articles}\\n\")\n",
    "            \n",
    "            # Segunda pasada: scraping real\n",
    "            print(\"Iniciando scraping paralelo de artículos...\")\n",
    "            future_to_page = {\n",
    "                executor.submit(self.scrape_page, page_num): page_num \n",
    "                for page_num in range(start_page, end_page + 1)\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=total_articles, desc=\"Progreso\") as pbar:\n",
    "                for future in as_completed(future_to_page):\n",
    "                    (page_num, page_articles), success_count = future.result()\n",
    "                    pbar.update(page_articles)\n",
    "                    processed_articles += success_count\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nScraping completado en {elapsed_time:.2f} segundos\")\n",
    "        print(f\"Artículos procesados exitosamente: {processed_articles}/{total_articles}\")\n",
    "        print(f\"Artículos fallidos: {total_articles - processed_articles}\")\n",
    "    \n",
    "    def save_to_csv(self, filename=\"plumas_libres_sociedad.csv\"):\n",
    "        \"\"\"Guarda los datos en CSV con codificación UTF-8\"\"\"\n",
    "        if not self.articles_data:\n",
    "            print(\"No hay datos para guardar\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "                fieldnames = ['url', 'titulo', 'articulo', 'fecha', 'autor']  # En minúsculas\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(self.articles_data)\n",
    "            \n",
    "            print(f\"\\nDatos guardados en {filename} (Total: {len(self.articles_data)} artículos)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError al guardar el archivo CSV: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Ejecución en Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PlumasLibresScraper()\n",
    "    \n",
    "    # Configuración (puedes ajustar estos valores)\n",
    "    start_page = 1\n",
    "    end_page = 47  # Recomiendo probar con pocas páginas primero\n",
    "    max_workers = 7  # Más bajo para Jupyter\n",
    "    \n",
    "    # Ejecutar scraping\n",
    "    scraper.scrape(start_page=start_page, end_page=end_page, max_workers=max_workers)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
