{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c5a7c2",
   "metadata": {},
   "source": [
    "# Test scraper La Voz del Sur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8d26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ====================================\n",
      "    SCRAPER LETRA FRÍA - VERSIÓN FINAL\n",
      "    Extracción completa garantizada\n",
      "    ====================================\n",
      "    \n",
      "\n",
      "Obteniendo artículos de: https://letrafria.com/category/region/\n",
      "Encontrados 0 artículos\n",
      "No se encontraron artículos. Terminando...\n",
      "\n",
      "Scraping completado. Artículos extraídos: 0\n",
      "Archivo guardado: letra_fria_completo_20250414_120720.csv\n",
      "\n",
      "Tiempo total: 0m 1s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuración\n",
    "BASE_URL = \"https://letrafria.com/\"\n",
    "CATEGORY_URL = \"https://letrafria.com/category/region/\"\n",
    "OUTPUT_FILE = f\"letra_fria_completo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "DELAY = random.uniform(2, 4)\n",
    "MAX_PAGES = 5  # Ajustar según necesidad\n",
    "\n",
    "# Headers optimizados\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'es-MX,es;q=0.9'\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Obtiene el HTML con manejo robusto de errores\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpia el texto preservando estructura\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def extract_article_data(article_url):\n",
    "    \"\"\"Extrae datos de artículo con selectores optimizados\"\"\"\n",
    "    print(f\"\\nExtrayendo: {article_url}\")\n",
    "    soup = get_soup(article_url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 1. Extraer título - selector confirmado\n",
    "        title = clean_text(soup.find('h1', class_='post-title').get_text())\n",
    "        \n",
    "        # 2. Extraer fecha - selectores específicos para Letra Fría\n",
    "        date_elem = soup.find('span', class_='post-date')  # Selector principal\n",
    "        if not date_elem:\n",
    "            date_elem = soup.find('time', class_='published')  # Alternativa\n",
    "            \n",
    "        date = clean_text(date_elem.get_text()) if date_elem else \"\"\n",
    "        \n",
    "        # 3. Extraer autor - selector específico confirmado\n",
    "        author_elem = soup.find('span', class_='post-author') or soup.find('a', class_='author-name')\n",
    "        author = clean_text(author_elem.get_text()) if author_elem else \"\"\n",
    "        \n",
    "        # 4. Extraer contenido - enfoque mejorado\n",
    "        content_div = soup.find('div', class_='post-content')\n",
    "        paragraphs = []\n",
    "        \n",
    "        if content_div:\n",
    "            # Eliminar elementos específicos no deseados\n",
    "            unwanted = ['sharedaddy', 'code-block', 'twitter-tweet', 'wp-block-embed']\n",
    "            for element in content_div.find_all(['script', 'style', 'iframe'] + unwanted):\n",
    "                element.decompose()\n",
    "            \n",
    "            for p in content_div.find_all(['p', 'h2', 'h3']):\n",
    "                text = clean_text(p.get_text())\n",
    "                if text and len(text.split()) > 2:\n",
    "                    paragraphs.append(text)\n",
    "        \n",
    "        content = '\\n\\n'.join(paragraphs) if paragraphs else \"Contenido no disponible\"\n",
    "        \n",
    "        print(f\"✓ Título: {title}\")\n",
    "        print(f\"✓ Fecha: {date}\")\n",
    "        print(f\"✓ Autor: {author}\")\n",
    "        print(f\"✓ Caracteres: {len(content)}\")\n",
    "        \n",
    "        return {\n",
    "            'titulo': title,\n",
    "            'articulo': content,\n",
    "            'fecha': date,\n",
    "            'autor': author,\n",
    "            'url': article_url,\n",
    "            'categoria': 'Región'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando artículo: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_article_links(page_url):\n",
    "    \"\"\"Obtiene enlaces con selectores robustos\"\"\"\n",
    "    print(f\"\\nObteniendo artículos de: {page_url}\")\n",
    "    soup = get_soup(page_url)\n",
    "    if not soup:\n",
    "        return []\n",
    "    \n",
    "    links = []\n",
    "    # Selector principal confirmado\n",
    "    articles = soup.select('article.item-post h2 a[href]')\n",
    "    \n",
    "    for article in articles:\n",
    "        link = urljoin(BASE_URL, article['href'])\n",
    "        if link not in links:\n",
    "            links.append(link)\n",
    "    \n",
    "    print(f\"Encontrados {len(links)} artículos\")\n",
    "    return links\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal con paginación mejorada\"\"\"\n",
    "    with open(OUTPUT_FILE, mode='w', encoding='utf-8', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['titulo', 'articulo', 'fecha', 'autor', 'url', 'categoria'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        page_num = 1\n",
    "        articles_scraped = 0\n",
    "        \n",
    "        while page_num <= MAX_PAGES:\n",
    "            page_url = CATEGORY_URL if page_num == 1 else f\"{CATEGORY_URL}page/{page_num}/\"\n",
    "            \n",
    "            article_links = get_article_links(page_url)\n",
    "            \n",
    "            if not article_links:\n",
    "                print(\"No se encontraron artículos. Terminando...\")\n",
    "                break\n",
    "            \n",
    "            for link in article_links:\n",
    "                article_data = extract_article_data(link)\n",
    "                if article_data:\n",
    "                    writer.writerow(article_data)\n",
    "                    csvfile.flush()\n",
    "                    articles_scraped += 1\n",
    "                \n",
    "                time.sleep(DELAY)\n",
    "            \n",
    "            page_num += 1\n",
    "            time.sleep(DELAY * 1.5)\n",
    "        \n",
    "        print(f\"\\nScraping completado. Artículos extraídos: {articles_scraped}\")\n",
    "        print(f\"Archivo guardado: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "    ====================================\n",
    "    SCRAPER LETRA FRÍA - VERSIÓN FINAL\n",
    "    Extracción completa garantizada\n",
    "    ====================================\n",
    "    \"\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\nTiempo total: {duration//60:.0f}m {duration%60:.0f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
