{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6d7d03",
   "metadata": {},
   "source": [
    "# Scraper funcional para páginas individuales de La Voz del Sur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6123f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# URL del artículo\n",
    "url = \"https://www.lavozdelsur.com.mx/laguneros-y-ciudad-guzman-participan-en-la-amdi-cup-en-tapalpa/\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 1. Extraer título (ya funcionaba bien)\n",
    "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'Título no encontrado'\n",
    "\n",
    "    # 2. Búsqueda MEJORADA de fecha - Estrategias múltiples\n",
    "    date = None\n",
    "    date_formats_to_try = [\n",
    "        '%Y-%m-%d',  # Formato ISO\n",
    "        '%d/%m/%Y',  # Formato latino\n",
    "        '%B %d, %Y'  # \"Enero 15, 2023\"\n",
    "    ]\n",
    "\n",
    "    # Estrategia 1: Buscar <time> con datetime\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    if time_tag:\n",
    "        datetime_attr = time_tag['datetime']\n",
    "        for fmt in date_formats_to_try:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(datetime_attr.split('T')[0], fmt)\n",
    "                date = date_obj.strftime('%d/%m/%Y')\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Estrategia 2: Buscar texto que parezca fecha\n",
    "    if not date:\n",
    "        possible_date_elements = soup.find_all(['span', 'div', 'p'], class_=True)\n",
    "        for element in possible_date_elements:\n",
    "            if 'date' in element.get('class', []):\n",
    "                text = element.get_text(strip=True)\n",
    "                for fmt in date_formats_to_try:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(text, fmt)\n",
    "                        date = date_obj.strftime('%d/%m/%Y')\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if date:\n",
    "                    break\n",
    "\n",
    "    # Estrategia 3: Buscar texto que contenga \"Publicado el\"\n",
    "    if not date:\n",
    "        for element in soup.find_all(string=lambda text: 'publicado' in text.lower()):\n",
    "            parent = element.parent\n",
    "            text = parent.get_text(strip=True)\n",
    "            date_part = text.split('Publicado el')[-1].strip().split()[0]\n",
    "            for fmt in date_formats_to_try:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_part, fmt)\n",
    "                    date = date_obj.strftime('%d/%m/%Y')\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    date = date or 'Fecha no encontrada'\n",
    "\n",
    "    # 3. Extraer contenido (ya funcionaba bien)\n",
    "    content_div = soup.find('article') or soup.find('div', class_='entry-content')\n",
    "    if content_div:\n",
    "        for element in content_div.find_all(['div', 'script', 'style', 'iframe']):\n",
    "            element.decompose()\n",
    "        content = '\\n'.join(p.get_text(strip=True) for p in content_div.find_all('p') if p.get_text(strip=True))\n",
    "    else:\n",
    "        content = 'Contenido no encontrado'\n",
    "\n",
    "    # Exportar a CSV\n",
    "    with open('articulo_la_voz_del_sur_mejorado.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Título', 'Fecha', 'Contenido'])\n",
    "        writer.writerow([title, date, content])\n",
    "\n",
    "    print(\"¡Extracción completada con éxito!\")\n",
    "    print(f\"Título: {title}\")\n",
    "    print(f\"Fecha encontrada: {date}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fa318",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
